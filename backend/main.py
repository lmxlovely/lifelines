"""
LifeLines Backend - äººç”Ÿè½¨è¿¹ API
FastAPI åç«¯æœåŠ¡ï¼Œæä¾›äººç”Ÿè½¨è¿¹é¢„æµ‹
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import os
import json
import time
import logging
from openai import OpenAI
from dotenv import load_dotenv

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ============ API é…ç½® ============
API_KEY = "sk-UADxiXLJiHHerZ4qXcimIT2Nve6s76EAouGrgZFfeccCXUjw"
BASE_URL = "https://aigc.x-see.cn/v1"

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

app = FastAPI(
    title="LifeLines API",
    description="é¢„æµ‹å¹¶å¯è§†åŒ–ä¸¤ä¸ªäººçš„äººç”Ÿè½¨è¿¹",
    version="1.0.0"
)

# CORS é…ç½® - å…è®¸å‰ç«¯åŸŸåè®¿é—®
origins = [
    "http://localhost:3000",
    "http://127.0.0.1:3000",
    "https://*.vercel.app",  # Vercel éƒ¨ç½²çš„å‰ç«¯
    # ç”Ÿäº§ç¯å¢ƒè¯·æ›¿æ¢ä¸ºä½ çš„å®é™…åŸŸå
    os.getenv("FRONTEND_URL", "http://localhost:3000"),
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å¼€å‘ç¯å¢ƒå…è®¸æ‰€æœ‰ï¼Œç”Ÿäº§ç¯å¢ƒè¯·é™åˆ¶
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============ æ•°æ®æ¨¡å‹ ============
class StoryRequest(BaseModel):
    name1: str
    name2: str


class StoryEvent(BaseModel):
    year: int
    event: str
    distance: int  # 0-100, 0ä¸ºåœ¨ä¸€èµ·, 100ä¸ºå®Œå…¨é™Œç”Ÿ
    emotion_score: int  # 0-10, ç”¨äºå‰ç«¯é…è‰²
    phase: Optional[str] = None  # é˜¶æ®µåç§°


class StoryResponse(BaseModel):
    events: List[StoryEvent]
    is_special: bool  # æ˜¯å¦æ˜¯å½©è›‹æ¨¡å¼
    theme: str  # ä¸»é¢˜: "destiny" | "default"


# ============ å½©è›‹æ•°æ®ï¼šæå½¦ & ææ¢¦ç¥¥çš„çœŸå®æ•…äº‹ ============
SPECIAL_STORY_LY_LMX = [
    {
        "year": 2018,
        "event": "é«˜ä¸€ä¸‹å­¦æœŸçš„å¤å¤©ï¼Œ6æœˆ25æ—¥ï¼Œè¿™ä¸ªå¹³å‡¡åˆç‰¹åˆ«çš„æ—¥å­ã€‚æå½¦å’Œææ¢¦ç¥¥åœ¨é‚£ä¸ªé—·çƒ­çš„æ•™å®¤é‡Œï¼Œç¡®å®šäº†å½¼æ­¤çš„å¿ƒæ„ã€‚æ²¡æœ‰è½°è½°çƒˆçƒˆçš„è¡¨ç™½ï¼Œåªæœ‰ä¸¤é¢—å¹´è½»çš„å¿ƒï¼Œåœ¨é’æ˜¥é‡Œæ‚„æ‚„é è¿‘ã€‚ä»æ­¤ï¼Œæ ¡å›­é‡Œå¤šäº†ä¸€å¯¹å½¢å½±ä¸ç¦»çš„èº«å½±ã€‚",
        "distance": 3,
        "emotion_score": 10,
        "phase": "ğŸ’• 6.25 åœ¨ä¸€èµ·"
    },
    {
        "year": 2019,
        "event": "é«˜äºŒé«˜ä¸‰çš„æ—¶å…‰ï¼Œæ˜¯æœ€çº¯ç²¹çš„ç”œèœœã€‚ä¸€èµ·ä¸Šè¯¾ã€ä¸€èµ·åƒé¥­ã€ä¸€èµ·æ™šè‡ªä¹ ã€‚å·å·åœ¨è¯¾æœ¬ä¸‹ä¼ çš„å°çº¸æ¡ï¼Œæ™šè‡ªä¹ åæ“åœºä¸Šçš„æ•£æ­¥ï¼Œéƒ½æˆäº†æœ€çè´µçš„å›å¿†ã€‚å¤‡æˆ˜é«˜è€ƒçš„æ—¥å­é‡Œï¼Œå½¼æ­¤æ˜¯æœ€æ¸©æš–çš„é™ªä¼´ã€‚é‚£æ—¶å€™è§‰å¾—ï¼Œåªè¦å’Œä½ åœ¨ä¸€èµ·ï¼Œæœªæ¥å°±ä»€ä¹ˆéƒ½ä¸æ€•ã€‚",
        "distance": 2,
        "emotion_score": 10,
        "phase": "ğŸŒ¸ é’æ˜¥æ­£å¥½"
    },
    {
        "year": 2020,
        "event": "é«˜è€ƒç»“æŸï¼Œæˆç»©æ­æ™“ã€‚å‘½è¿å¼€äº†ä¸€ä¸ªç©ç¬‘â€”â€”æå½¦å»äº†å®‰é˜³ï¼Œææ¢¦ç¥¥å»äº†å—äº¬ã€‚ä»æ²³å—åˆ°æ±Ÿè‹ï¼Œ800å¤šå…¬é‡Œçš„è·ç¦»ï¼Œä»æ­¤æ€å¿µè¦è·¨è¶Šå¤§åŠä¸ªä¸­å›½ã€‚ä¸´åˆ«é‚£å¤©ï¼Œä¸¤äººéƒ½æ²¡å“­ï¼Œåªæ˜¯ç´§ç´§æ¡ç€å¯¹æ–¹çš„æ‰‹è¯´ï¼š'ç­‰æˆ‘'ã€‚",
        "distance": 35,
        "emotion_score": 7,
        "phase": "ğŸš‚ å¼‚åœ°å¼€å§‹"
    },
    {
        "year": 2021,
        "event": "å¤§ä¸€åˆ°å¤§äºŒï¼Œå¼‚åœ°æ‹è¿›å…¥æœ€éš¾ç†¬çš„é˜¶æ®µã€‚å®‰é˜³åˆ°å—äº¬ï¼Œæ²¡æœ‰ç›´è¾¾çš„é«˜é“ï¼Œæ¯ä¸€æ¬¡è§é¢éƒ½è¦ç²¾å¿ƒè®¡åˆ’ã€‚è§†é¢‘é€šè¯ä»æ¯å¤©å˜æˆéš”å¤©ï¼Œè¯é¢˜ä»åˆ†äº«æ—¥å¸¸å˜æˆäº†æ²‰é»˜ã€‚ç–«æƒ…è®©è§é¢å˜å¾—æ›´åŠ å¥¢ä¾ˆï¼Œæ€å¿µåœ¨è·ç¦»ä¸­æ…¢æ…¢å‘é…µæˆç„¦è™‘ã€‚",
        "distance": 50,
        "emotion_score": 5,
        "phase": "ğŸ“± æ€å¿µä¸ç­‰å¾…"
    },
    {
        "year": 2022,
        "event": "å¤§ä¸‰ï¼Œææ¢¦ç¥¥å¼€å§‹å‡†å¤‡è€ƒç ”ï¼Œå‹åŠ›ä¸æ—¥ä¿±å¢ã€‚æå½¦è¯•å›¾ç†è§£å’Œæ”¯æŒï¼Œä½†ä¸¤ä¸ªäººçš„ç”Ÿæ´»èŠ‚å¥è¶Šæ¥è¶Šä¸åŒæ­¥ã€‚å¥¹åœ¨å›¾ä¹¦é¦†åˆ·é¢˜åˆ°æ·±å¤œï¼Œä»–åœ¨ç­‰ç€é‚£ä¸ªè¶Šæ¥è¶Šæ™šçš„æ™šå®‰ã€‚äº‰åµå˜å¤šäº†ï¼Œå†·æˆ˜ä¹Ÿå˜å¤šäº†ã€‚æ›¾ç»æ— è¯ä¸è°ˆçš„ä¸¤ä¸ªäººï¼Œå¼€å§‹ä¸çŸ¥é“è¯¥è¯´ä»€ä¹ˆã€‚",
        "distance": 60,
        "emotion_score": 4,
        "phase": "ğŸ’” è£‚ç—•æ¸ç”Ÿ"
    },
    {
        "year": 2023,
        "event": "å¤§å››ï¼Œææ¢¦ç¥¥è€ƒç ”ç»“æŸåçš„æŸä¸ªå¤œæ™šï¼Œä¸€åœºç§¯å‹å·²ä¹…çš„äº‰åµç»ˆäºçˆ†å‘ã€‚é‚£äº›å§”å±ˆã€é‚£äº›ä¸ç†è§£ã€é‚£äº›å¼‚åœ°çš„å¿ƒé…¸ï¼Œå…¨éƒ½åŒ–æˆäº†ä¼¤äººçš„è¯ã€‚æœ€åï¼Œä¸¤ä¸ªäººéƒ½æ²‰é»˜äº†ã€‚'æˆ‘ä»¬...åˆ†å¼€å§ã€‚' äº”å¹´çš„æ„Ÿæƒ…ï¼Œåœ¨é‚£ä¸ªå¯’å†·çš„å†¬å¤œç”»ä¸Šäº†å¥å·ã€‚",
        "distance": 95,
        "emotion_score": 1,
        "phase": "ğŸ’” åˆ†æ‰‹"
    },
    {
        "year": 2024,
        "event": "æå½¦æ¯•ä¸šåå¼€å§‹å·¥ä½œï¼Œææ¢¦ç¥¥å¦‚æ„¿è€ƒä¸Šäº†ç ”ç©¶ç”Ÿã€‚ä¸¤ä¸ªäººçš„ç”Ÿæ´»ï¼Œå½»åº•å˜æˆäº†ä¸¤æ¡å¹³è¡Œçº¿ã€‚åˆ æ‰äº†æœ‹å‹åœˆï¼Œå±è”½äº†å…±åŒå¥½å‹çš„æ¶ˆæ¯ï¼Œå‡è£…å¯¹æ–¹å·²ç»ä¸å­˜åœ¨ã€‚å¯æ˜¯æ·±å¤œå¤±çœ çš„æ—¶å€™ï¼Œè¿˜æ˜¯ä¼šå¿ä¸ä½ç‚¹å¼€é‚£äº›èˆä¸å¾—åˆ çš„èŠå¤©è®°å½•ã€‚",
        "distance": 85,
        "emotion_score": 2,
        "phase": "ğŸ‘¤ å„è‡ªå¤©æ¶¯"
    },
    {
        "year": 2025,
        "event": "æ—¶é—´æ˜¯æœ€å¥½çš„è§£è¯ã€‚æå½¦åœ¨å·¥ä½œä¸­æ‰¾åˆ°äº†è‡ªå·±çš„èŠ‚å¥ï¼Œææ¢¦ç¥¥çš„ç ”ç©¶ç”Ÿç”Ÿæ´»ä¹Ÿæ¸å…¥ä½³å¢ƒã€‚æ›¾ç»é‚£äº›åˆ»éª¨é“­å¿ƒçš„ç—›ï¼Œæ…¢æ…¢å˜æˆäº†å¶å°”æƒ³èµ·æ—¶å˜´è§’çš„ä¸€ä¸è‹¦ç¬‘ã€‚ä»–ä»¬éƒ½åœ¨å­¦ç€å’Œè¿‡å»å’Œè§£ï¼Œå’Œè‡ªå·±å’Œè§£ã€‚",
        "distance": 70,
        "emotion_score": 4,
        "phase": "ğŸŒ± å„è‡ªæˆé•¿"
    },
    {
        "year": 2026,
        "event": "2026å¹´çš„æŸä¸€å¤©ï¼Œä¸€æ¡å¾®ä¿¡æ¶ˆæ¯æ‰“ç ´äº†ä¸¤å¹´å¤šçš„æ²‰é»˜ã€‚ä¸çŸ¥é“æ˜¯è°å…ˆé¼“èµ·çš„å‹‡æ°”ï¼Œä½†é‚£å¥'æœ€è¿‘è¿˜å¥½å—'è®©ä¸¤ä¸ªäººéƒ½çº¢äº†çœ¼çœ¶ã€‚ä»å°å¿ƒç¿¼ç¿¼çš„å¯’æš„ï¼Œåˆ°æ·±å¤œé‡Œè¯´ä¸å®Œçš„è¯ã€‚åŸæ¥è¿™äº›å¹´ï¼Œå½¼æ­¤éƒ½æ²¡æœ‰çœŸæ­£æ”¾ä¸‹è¿‡ã€‚",
        "distance": 40,
        "emotion_score": 7,
        "phase": "ğŸ’¬ é‡æ–°è”ç³»"
    },
    {
        "year": 2026,
        "event": "ä»–ä»¬å¼€å§‹åœ¨ç½‘ä¸Šåˆ†äº«å„è‡ªçš„ç”Ÿæ´»ï¼ŒèŠå·¥ä½œã€èŠç†æƒ³ã€èŠè¿™äº›å¹´çš„æˆé•¿ä¸é—æ†¾ã€‚è™½ç„¶è¿˜æ²¡è§é¢ï¼Œä½†é‚£ç§ç†Ÿæ‚‰çš„æ„Ÿè§‰ï¼Œé‚£ç§åªæœ‰å¯¹æ–¹æ‰èƒ½ç»™çš„å®‰å¿ƒï¼Œåˆæ‚„æ‚„å›æ¥äº†ã€‚å‘½è¿çš„é½¿è½®æ­£åœ¨ç¼“ç¼“è½¬åŠ¨ï¼Œæ•…äº‹è¿˜åœ¨ç»§ç»­... âœ¨",
        "distance": 25,
        "emotion_score": 8,
        "phase": "ğŸ’« å‘½è¿é‡å¯"
    },
    {
        "year": 2027,
        "event": "ä¹Ÿè®¸æœ‰ä¸€å¤©ï¼Œä»–ä»¬ä¼šåœ¨æŸä¸ªåŸå¸‚é‡é€¢ã€‚ä¹Ÿè®¸ä¼šä¸€èµ·å–æ¯å’–å•¡ï¼ŒèŠèŠè¿™äº›å¹´é”™è¿‡çš„æ—¶å…‰ã€‚ä¹Ÿè®¸ä¼šå†æ¬¡ç‰µèµ·å¯¹æ–¹çš„æ‰‹ï¼Œä¹Ÿè®¸åªæ˜¯ç›¸è§†ä¸€ç¬‘ã€‚ä½†æ— è®ºç»“å±€å¦‚ä½•ï¼Œè¿™æ®µæ•…äº‹éƒ½å·²ç»æ˜¯å½¼æ­¤ç”Ÿå‘½ä¸­ï¼Œæœ€ç‰¹åˆ«çš„å­˜åœ¨ã€‚",
        "distance": 15,
        "emotion_score": 9,
        "phase": "ğŸŒŸ æœªå®Œå¾…ç»­"
    },
    {
        "year": 2028,
        "event": "å…œå…œè½¬è½¬ï¼Œè¿˜æ˜¯ä½ ã€‚é‚£äº›å¹´å°‘æ—¶è®¸ä¸‹çš„æ‰¿è¯ºï¼Œé‚£äº›ä»¥ä¸ºå†ä¹Ÿå›ä¸å»çš„è¿‡å¾€ï¼Œéƒ½åœ¨æ—¶é—´çš„æ²‰æ·€ä¸­å˜æˆäº†å‘½ä¸­æ³¨å®šã€‚æœ‰äº›äººèµ°ç€èµ°ç€å°±æ•£äº†ï¼Œæœ‰äº›äººæ•£äº†è¿˜ä¼šå†ç›¸é‡ã€‚è€Œä½ ï¼Œæ˜¯æˆ‘ç»‘äº†ä¸€å¤§åœˆï¼Œè¿˜æ˜¯æƒ³è¦å›åˆ°çš„åŸç‚¹ã€‚ğŸ’•",
        "distance": 0,
        "emotion_score": 10,
        "phase": "ğŸ’‘ å…œå…œè½¬è½¬ï¼Œè¿˜æ˜¯ä½ "
    }
]


def is_special_couple(name1: str, name2: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹æ®Šå½©è›‹ç»„åˆ"""
    names = {name1.strip(), name2.strip()}
    # æ”¯æŒå¤šç§å†™æ³•
    special_variants = [
        {"æå½¦", "ææ¢¦ç¥¥"},
        {"æå½¦", "æå¤¢ç¥¥"},
        {"liyan", "limengxiang"},
        {"LY", "LMX"},
        {"ly", "lmx"},
        {"å½¦", "æ¢¦ç¥¥"},
    ]
    # ä¸åŒºåˆ†å¤§å°å†™æ¯”è¾ƒ
    names_lower = {n.lower() for n in names}
    for variant in special_variants:
        if {v.lower() for v in variant} == names_lower:
            return True
    return False


# ============ LLM è°ƒç”¨ï¼ˆé€šç”¨æ¨¡å¼ï¼‰ ============
def get_completion_from_gpt4(prompt: str, system_prompt: str, max_retries: int = 3, delay: int = 3):
    """
    ä½¿ç”¨ GPT-4 API è·å–å›ç­”ï¼Œå¸¦é‡è¯•æœºåˆ¶
    """
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4o-2024-08-06",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.8,
                max_tokens=2000,
            )
            return response.choices[0].message.content
        except Exception as e:
            error_message = str(e).lower()
            logging.error(f"API call failed on attempt {attempt + 1}/{max_retries}: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é¢åº¦æˆ–é€Ÿç‡é™åˆ¶é”™è¯¯
            if "rate limit" in error_message or "quota" in error_message:
                logging.error("Quota or rate limit error detected.")
                return None
            
            if attempt < max_retries - 1:
                logging.info(f"Retrying in {delay} seconds...")
                time.sleep(delay)
            else:
                logging.error("Max retries reached. Returning None.")
                return None
    return None


async def generate_story_with_llm(name1: str, name2: str) -> List[dict]:
    """
    ä½¿ç”¨ LLM ç”Ÿæˆäººç”Ÿæ•…äº‹
    """
    system_prompt = """ä½ æ˜¯ä¸€ä½æµªæ¼«çš„å‘½è¿å™è¿°è€…ã€‚æ ¹æ®ä¸¤ä¸ªäººçš„åå­—ï¼Œåˆ›ä½œä¸€æ®µè·¨è¶Š10å¹´çš„äººç”Ÿæ•…äº‹ã€‚

è¦æ±‚ï¼š
1. æ•…äº‹å¿…é¡»æœ‰èµ·ä¼ï¼šç›¸è¯† -> ç›¸çŸ¥ -> çƒ­æ‹ -> æ³¢æŠ˜ -> ç»“å±€ï¼ˆå¯ä»¥æ˜¯åœ†æ»¡æˆ–é—æ†¾ï¼‰
2. æ¯ä¸ªäº‹ä»¶çš„ distance èŒƒå›´æ˜¯ 0-100ï¼ˆ0=åœ¨ä¸€èµ·ï¼Œ100=å®Œå…¨é™Œç”Ÿï¼‰
3. emotion_score èŒƒå›´æ˜¯ 0-10ï¼ˆå½±å“é¢œè‰²ï¼Œ10æœ€æ¸©æš–ï¼Œ0æœ€å†·ï¼‰
4. è¿”å›ä¸¥æ ¼çš„ JSON æ•°ç»„æ ¼å¼
5. åŒ…å« 8-12 ä¸ªäº‹ä»¶èŠ‚ç‚¹
6. æ•…äº‹è¦æ„Ÿäººã€æœ‰ç»†èŠ‚ã€æœ‰ç”»é¢æ„Ÿ

è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
[
    {"year": 2020, "event": "æè¿°...", "distance": 50, "emotion_score": 5, "phase": "é˜¶æ®µå"},
    ...
]

åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦å…¶ä»–ä»»ä½•æ–‡å­—æˆ–markdownæ ‡è®°ã€‚"""

    user_prompt = f"è¯·ä¸º {name1} å’Œ {name2} åˆ›ä½œä¸€æ®µå‘½è¿äº¤ç»‡çš„äººç”Ÿæ•…äº‹ã€‚ä»2018å¹´å¼€å§‹ï¼Œåˆ°2028å¹´ç»“æŸã€‚"

    try:
        result = get_completion_from_gpt4(user_prompt, system_prompt)
        if result:
            # å°è¯•è§£æ JSONï¼Œå¤„ç†å¯èƒ½çš„ markdown ä»£ç å—
            clean_result = result.strip()
            if "```json" in clean_result:
                clean_result = clean_result.split("```json")[1].split("```")[0]
            elif "```" in clean_result:
                clean_result = clean_result.split("```")[1].split("```")[0]
            
            return json.loads(clean_result.strip())
    except json.JSONDecodeError as e:
        logging.error(f"JSON parse error: {e}")
    except Exception as e:
        logging.error(f"Error generating story: {e}")
    
    # é™çº§åˆ°é»˜è®¤æ•…äº‹
    return generate_default_story(name1, name2)


def generate_default_story(name1: str, name2: str) -> List[dict]:
    """ç”Ÿæˆé»˜è®¤çš„é€šç”¨æ•…äº‹ï¼ˆæ— éœ€ LLMï¼‰"""
    current_year = 2024
    return [
        {
            "year": current_year - 6,
            "event": f"{name1}å’Œ{name2}åœ¨ä¸€ä¸ªæ™®é€šçš„æ—¥å­é‡Œç›¸é‡ã€‚ä¹Ÿè®¸æ˜¯å’–å•¡åº—çš„å¶é‡ï¼Œä¹Ÿè®¸æ˜¯æœ‹å‹çš„ä»‹ç»ã€‚å‘½è¿çš„ä¸çº¿å¼€å§‹æ‚„æ‚„ç¼–ç»‡ã€‚",
            "distance": 70,
            "emotion_score": 5,
            "phase": "åˆé‡"
        },
        {
            "year": current_year - 5,
            "event": f"ç›¸å¤„æ—¥ä¹…ï¼Œä¸¤äººå‘ç°å½¼æ­¤æœ‰ç€ç›¸ä¼¼çš„çµé­‚ã€‚{name1}å–œæ¬¢{name2}ç¬‘èµ·æ¥çš„æ ·å­ï¼Œ{name2}æ¬£èµ{name1}è®¤çœŸçš„æ¨¡æ ·ã€‚",
            "distance": 40,
            "emotion_score": 7,
            "phase": "ç›¸çŸ¥"
        },
        {
            "year": current_year - 4,
            "event": "æ„Ÿæƒ…å‡æ¸©ï¼Œç¡®å®šäº†æ‹çˆ±å…³ç³»ã€‚é‚£ä¸ªå¤å¤©çš„æ¯ä¸€å¸§ç”»é¢éƒ½é—ªé—ªå‘å…‰ã€‚",
            "distance": 15,
            "emotion_score": 9,
            "phase": "çƒ­æ‹"
        },
        {
            "year": current_year - 3,
            "event": "ç”Ÿæ´»çš„çç¢å¼€å§‹è€ƒéªŒè¿™æ®µæ„Ÿæƒ…ã€‚å·¥ä½œçš„å‹åŠ›ã€å®¶äººçš„æœŸå¾…ã€æœªæ¥çš„ä¸ç¡®å®š...äº‰åµå¼€å§‹å‡ºç°ã€‚",
            "distance": 35,
            "emotion_score": 5,
            "phase": "è€ƒéªŒ"
        },
        {
            "year": current_year - 2,
            "event": "ç»å†äº†æœ€è‰°éš¾çš„æ—¶åˆ»ï¼Œä¸¤äººå­¦ä¼šäº†æ²Ÿé€šå’Œç†è§£ã€‚åŸæ¥ï¼Œçˆ±æƒ…ä¸åªæ˜¯å¿ƒåŠ¨ï¼Œæ›´æ˜¯é€‰æ‹©ä¸€èµ·é¢å¯¹é£é›¨ã€‚",
            "distance": 20,
            "emotion_score": 7,
            "phase": "æˆé•¿"
        },
        {
            "year": current_year - 1,
            "event": "æºæ‰‹èµ°è¿‡äº†è¿™ä¹ˆå¤šï¼Œæœªæ¥çš„è·¯ä¾ç„¶å¾ˆé•¿ã€‚ä½†åªè¦æœ‰ä½ åœ¨èº«è¾¹ï¼Œå°±æœ‰å‹‡æ°”é¢å¯¹ä¸€åˆ‡ã€‚",
            "distance": 10,
            "emotion_score": 8,
            "phase": "åšå®š"
        },
        {
            "year": current_year,
            "event": f"æ•…äº‹è¿˜åœ¨ç»§ç»­ã€‚{name1}å’Œ{name2}çš„äººç”Ÿè½¨è¿¹ï¼Œæ­£åœ¨ä¹¦å†™æ–°çš„ç¯‡ç« ...",
            "distance": 5,
            "emotion_score": 9,
            "phase": "å½“ä¸‹"
        }
    ]


# ============ API ç«¯ç‚¹ ============
@app.get("/")
async def root():
    """å¥åº·æ£€æŸ¥"""
    return {
        "message": "LifeLines API is running",
        "version": "1.0.0",
        "docs": "/docs"
    }


@app.post("/api/predict_story", response_model=StoryResponse)
async def predict_story(request: StoryRequest):
    """
    é¢„æµ‹ä¸¤ä¸ªäººçš„äººç”Ÿè½¨è¿¹æ•…äº‹
    
    - **name1**: ç¬¬ä¸€ä¸ªäººçš„åå­—
    - **name2**: ç¬¬äºŒä¸ªäººçš„åå­—
    
    è¿”å›ä»–ä»¬çš„å‘½è¿æ•…äº‹æ—¶é—´çº¿
    """
    
    name1 = request.name1.strip()
    name2 = request.name2.strip()
    
    if not name1 or not name2:
        raise HTTPException(status_code=400, detail="è¯·è¾“å…¥ä¸¤ä¸ªäººçš„åå­—")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹æ®Šå½©è›‹
    if is_special_couple(name1, name2):
        return StoryResponse(
            events=[StoryEvent(**event) for event in SPECIAL_STORY_LY_LMX],
            is_special=True,
            theme="destiny"
        )
    
    # é€šç”¨æ¨¡å¼ï¼šè°ƒç”¨ LLM ç”Ÿæˆæ•…äº‹
    try:
        story_data = await generate_story_with_llm(name1, name2)
        events = [StoryEvent(**event) for event in story_data]
        return StoryResponse(
            events=events,
            is_special=False,
            theme="default"
        )
    except Exception as e:
        print(f"Error generating story: {e}")
        # é™çº§åˆ°é»˜è®¤æ•…äº‹
        story_data = generate_default_story(name1, name2)
        events = [StoryEvent(**event) for event in story_data]
        return StoryResponse(
            events=events,
            is_special=False,
            theme="default"
        )


@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy", "service": "lifelines-api"}


# ============ æœ¬åœ°å¼€å‘å¯åŠ¨ ============
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
